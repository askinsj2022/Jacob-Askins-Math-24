# -*- coding: utf-8 -*-
"""Jacob Askins Math 24 Lab 10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/115Cw4PZW8UeXkNolQnCSStUMDz-mx6BB

## Setup

### Imports
"""

import numpy as np
import matplotlib.pyplot as plt
import urllib.request
from PIL import Image
from imageio import *
import torch
from skimage.transform import resize
from mpl_toolkits.axes_grid1.axes_rgb import make_rgb_axes, RGBAxes
from torchvision.models import *
from torchvision.datasets import MNIST,KMNIST,FashionMNIST
from skimage.util import montage

#Imports libraries for numerical operations, plotting, image processing, and machine learning.

!pip install wandb
import wandb as wb

#Installs Weights & Biases (wandb), then imports it as 'wb' for experiment tracking.

def plot(x):
    if type(x) == torch.Tensor :
        x = x.cpu().detach().numpy()

    fig, ax = plt.subplots()
    im = ax.imshow(x, cmap = 'gray')
    ax.axis('off')
    fig.set_size_inches(5, 5)
    plt.show()

    #Defines a function to plot a tensor or array as a grayscale image, hiding axes.

def montage_plot(x):
    x = np.pad(x, pad_width=((0, 0), (1, 1), (1, 1)), mode='constant', constant_values=0)
    plot(montage(x))

    #Adds padding around images, then creates and plots a montage of the padded images.

b = 1000

def get_batch(mode):
    if mode == "train":
        r = np.random.randint(X.shape[0]-b)
        x = X[r:r+b,:]
        y = Y[r:r+b]
    elif mode == "test":
        r = np.random.randint(X_test.shape[0]-b)
        x = X_test[r:r+b,:]
        y = Y_test[r:r+b]
    return x,y

    #Selects a random batch from the dataset for training or testing based on the mode.

"""## MNIST

### Load Data
"""

# #MNIST
train_set = MNIST('./data', train=True, download=True)
test_set  = MNIST('./data', train=False, download=True)

#KMNIST
# train_set = KMNIST('./data', train=True, download=True)
# test_set =  KMNIST('./data', train=False, download=True)

# Fashion MNIST
# train_set = FashionMNIST('./data', train=True, download=True)
# test_set =  FashionMNIST('./data', train=False, download=True)

X = train_set.data.numpy()
X_test = test_set.data.numpy()
Y = train_set.targets.numpy()
Y_test = test_set.targets.numpy()

X = X[:,None,:,:]/255
X_test = X_test[:,None,:,:]/255

#Converts train/test data and labels to numpy arrays, normalizes images, and adds a channel dimension.

X.shape

#Prints the shape of 'X'.

Y[50000]

#Returns the label of the 50001st sample in the dataset.

plot(X[50000,0,:,:])

#This code plots the 50,001st image in the dataset `X` as a grayscale image.

Y[100]

##Returns the label of the 101st sample in the dataset.

X.shape

#Prints the shape of 'X'.

X[0:25,0,:,:].shape

#Shape is (25, height, width) for the first 25 samples' first channel.

montage_plot(X[125:150,0,:,:])

#Creates a 5x5 matrix of 25 images of numbers.

X.shape[0]

#Prints the shape of 'X'.

X_test.shape
#Prints the shape of 'X_test'.

X.shape[0]

#Prints the shape of 'X'.

X_test.shape[0]

#Prints the shape of 'X_test'.

def GPU(data):
    return torch.tensor(data, requires_grad=True, dtype=torch.float, device=torch.device('cuda'))

def GPU_data(data):
    return torch.tensor(data, requires_grad=False, dtype=torch.float, device=torch.device('cuda'))

    #Creates tensors on GPU with gradientsand without gradients for fixed data.

X = GPU_data(X)
Y = GPU_data(Y)
X_test = GPU_data(X_test)
Y_test = GPU_data(Y_test)

#Converts training and testing data and labels to GPU tensors without gradients.

X = X.reshape(X.shape[0],784)
X_test = X_test.reshape(X_test.shape[0],784)

#Reshapes the datasets to have 784 features per sample, flattening images.

X.shape

"""
### Classifier
"""

x,y = get_batch('train')

#Selects a random batch of 1000 samples from the training dataset `X` and their labels `Y`.

x.shape

#Prints size of batch of flatten images.

plot(x[0].reshape(28,28))

#Plots the first image in the batch `x`, reshaped back to 28x28 pixels, as grayscale.

plot(x[1].reshape(28,28))

#This code plots the second image in the batch `x`, reshaped to 28x28, in grayscale.

plot(x[2].reshape(28,28))

#This code plots the third image in the batch `x`, reshaped to 28x28, in grayscale.

y[:10]

#Retrieves the first 10 labels from the batch `y`.

W = GPU(np.random.randn(784,10))

#Initializes a GPU tensor `W` with gradients, using random values of shape (784, 10).



x.shape, W.shape

torch.matmul(x,W).shape

#Matrix multiplication and prints size of matrix.

(x@W).shape

#Matrix multiplication and prints size of matrix.

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# x@W
# 
# #Measures performance of matrix multiplication between `x` and `W` on GPU.

x@W

y2 = x@W

plot(y2[:50])

y

y.shape

def one_hot(y):
    y2 = GPU_data(torch.zeros((y.shape[0],10)))
    for i in range(y.shape[0]):
        y2[i,int(y[i])] = 1
    return y2

    #Converts labels `y` to one-hot encoded format on GPU without gradients.

one_hot(y)

#Creates a one-hot encoded tensor for labels `y` on GPU, suitable for model training.

torch.argmax(y2,1)

#Returns indices of maximum values in `y2` along dimension 1, reverting one-hot encoding.

torch.sum(y == torch.argmax(y2,1))/b

#Calculates the fraction of correctly predicted labels by comparing `y` with the argmax of `y2`, divided by batch size `b`.

X.shape

X@W

torch.argmax(X@W,1)

#Computes indices of maximum values across dimension 1 of matrix product `X@W`, indicating predicted classes.

Y

torch.sum(torch.argmax(X@W,1) == Y)/60000

#Calculates accuracy of predictions by comparing predicted classes with true labels `Y`, normalized by dataset size.

X@W

W.shape

W[:,0].shape

plot(W[:,0].reshape(28,28))

#Plots the first column of weight matrix `W`, reshaped to 28x28, as a grayscale image.

plot(W[:,2].reshape(28,28))

#This code visualizes the third column of the weight matrix `W`, reshaped into a 28x28 image, in grayscale.

W.shape

(W.T).shape

montage_plot((W.T).reshape(10,28,28).cpu().detach().numpy())

#Creates a montage of the transpose of weight matrix `W`, reshaped into 10 28x28 images, and plots them.

def softmax(x):
    s1 = torch.exp(x - torch.max(x,1)[0][:,None])
    s = s1 / s1.sum(1)[:,None]
    return s

    #Defines a softmax function that normalizes input `x` into a probability distribution over predicted output classes.

def cross_entropy(outputs, labels):
    return -torch.sum(softmax(outputs).log()[range(outputs.size()[0]), labels.long()])/outputs.size()[0]

    #Computes the cross-entropy loss between softmax-transformed outputs and true labels for classification tasks.

def acc(out,y):
    return (torch.sum(torch.max(out,1)[1] == y).item())/y.shape[0]

    #Calculates accuracy by comparing the predicted class (max output) with true labels, normalized by total samples.

def get_batch(mode):
    b = c.b
    if mode == "train":
        r = np.random.randint(X.shape[0]-b)
        x = X[r:r+b,:]
        y = Y[r:r+b]
    elif mode == "test":
        r = np.random.randint(X_test.shape[0]-b)
        x = X_test[r:r+b,:]
        y = Y_test[r:r+b]
    return x,y

    #Selects a random batch for training or testing based on `mode`, using `X`, `Y`, `X_test`, and `Y_test`.

def model(x,w):

    return x@w[0]

    #Defines a simple linear model function that performs matrix multiplication between input `x` and weight `w[0]`.

def gradient_step(w):

    w[0].data = w[0].data - c.L*w[0].grad.data

    w[0].grad.data.zero_()

    #Updates the weight `w[0]` by subtracting the product of a learning rate `c.L` and its gradient, then resets the gradient.

def make_plots():

    acc_train = acc(model(x,w),y)

    xt,yt = get_batch('test')

    acc_test = acc(model(xt,w),yt)

    wb.log({"acc_train": acc_train, "acc_test": acc_test})

    #Calculates training and test accuracies, then logs them using Weights & Biases (wandb).

def Truncated_Normal(size):

    u1 = torch.rand(size)*(1-np.exp(-2)) + np.exp(-2)
    u2 = torch.rand(size)
    z  = torch.sqrt(-2*torch.log(u1)) * torch.cos(2*np.pi*u2)

    return z

#Generates samples from a truncated normal distribution using the Box-Muller transform.

for run in range(3):

    wb.init(project="Simple_Linear_SGD_123");
    c = wb.config

    c.L = 0.1
    c.b = 1024
    c.epochs = 10000

    w = [GPU(Truncated_Normal((784,10)))]

    for i in range(c.epochs):

        x,y = get_batch('train')

        out = model(x,w)

        loss = cross_entropy(softmax(out),y)

        loss.backward()

        gradient_step(w)

        make_plots()

        if (i+1) % 10000 == 0: montage_plot((w[0].T).reshape(10,28,28).cpu().detach().numpy())

#Runs training loop 3 times, updating weights and logging metrics, plots weights every 10,000 epochs.

for run in range(100):

    wb.init(project="Simple_Linear_Adam_2");
    c = wb.config

    c.L = 0.01
    c.b = 1024
    c.epochs = 100000

    w = [GPU(Truncated_Normal((784,10)))]

    optimizer = torch.optim.Adam(w, lr=c.L)

    for i in range(c.epochs):

        x,y = get_batch('train')

        loss = cross_entropy(softmax(model(x,w)),y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        wb.log({"loss": loss})

        make_plots()

        if i % 10000 == 0 : montage_plot((w[0].T).reshape(10,28,28).cpu().detach().numpy())

        #Runs a training loop 100 times with Adam optimizer, logging loss and plots, updating and visualizing weights periodically.

"""
### Autoencoder
"""

def get_batch(mode):
    b = 1024
    if mode == "train":
        r = np.random.randint(X.shape[0]-b)
        x = X[r:r+b,:]
        y = Y[r:r+b]
    elif mode == "test":
        r = np.random.randint(X_test.shape[0]-b)
        x = X_test[r:r+b,:]
        y = Y_test[r:r+b]
    return x,y

#Retrieves a random batch of size 1024 for either training or testing, based on the mode.

X = X.reshape(X.shape[0],1,28,28)
X_test = X_test.reshape(X_test.shape[0],1,28,28)

#Reshapes datasets `X` and `X_test` to include a channel dimension, suitable for convolutional neural networks.

import torchvision
from torch.nn.functional import *

#Imports PyTorch's torchvision library and functional API for deep learning operations.

X = torchvision.transforms.functional.normalize(X,0.5,0.5)
X_test = torchvision.transforms.functional.normalize(X_test,0.5,0.5)

#Normalizes `X` and `X_test` datasets with mean 0.5 and standard deviation 0.5 using torchvision.

def Encoder(x,w):
    x = relu(conv2d(x,w[0], stride=(2, 2), padding=(1, 1)))
    x = relu(conv2d(x,w[1], stride=(2, 2), padding=(1, 1)))
    x = x.view(x.size(0), 6272)
    x = linear(x,w[2])
    return x

#Defines an encoder function using ReLU activation and convolutions, then reshapes and applies a linear transformation.

def Decoder(x,w):
    x = linear(x,w[3])
    x = x.view(x.size(0), 128, 7, 7)
    x = relu(conv_transpose2d(x,w[4], stride=(2, 2), padding=(1, 1)))
    x = torch.tanh(conv_transpose2d(x,w[5], stride=(2, 2), padding=(1, 1)))
    return x

    #Defines a decoder function that reverses the encoding process using linear layers, reshaping, and transposed convolutions with activations.

def Autoencoder(x,w):
    return Decoder(Encoder(x,w),w)

    #Composes the encoder and decoder functions into an autoencoder, processing input `x` through both stages using weights `w`.

num_steps = 1000
batch_size = 512
learning_rate = 1e-3

#Sets the number of training steps to 1000, batch size to 512, and learning rate to 0.001 for training.

from scipy import stats
import numpy as np
import matplotlib.pyplot as plt
import urllib.request
from PIL import Image
from imageio import *
import torch
from skimage.transform import resize
from mpl_toolkits.axes_grid1.axes_rgb import make_rgb_axes, RGBAxes
from torchvision.models import *
from torchvision.datasets import MNIST,KMNIST,FashionMNIST
from skimage.util import montage

#Imports libraries for statistical analysis, numerical operations, image processing, and deep learning models and datasets.

def randn_trunc(s): #Truncated Normal Random Numbers
    mu = 0
    sigma = 0.1
    R = stats.truncnorm((-2*sigma - mu) / sigma, (2*sigma - mu) / sigma, loc=mu, scale=sigma)
    return R.rvs(s)

    #Defines a function to generate random numbers from a truncated normal distribution with specified shape `s`, mean `mu`, and standard deviation `sigma`, limited to within two standard deviations from the mean.

#Encode
w0 = GPU(randn_trunc((64,1,4,4)))
w1 = GPU(randn_trunc((128,64,4,4)))
w2 = GPU(randn_trunc((10,6272)))
#Decode
w3 = GPU(randn_trunc((6272,10)))
w4 = GPU(randn_trunc((128,64,4,4)))
w5 = GPU(randn_trunc((64,1,4,4)))

w = [w0,w1,w2,w3,w4,w5]

optimizer = torch.optim.Adam(params=w, lr=learning_rate)

for i in range(num_steps):

    x_real,y = get_batch('train')

    x_fake = Autoencoder(x_real,w)

    loss = torch.mean((x_fake - x_real)**2)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if i % 100 == 0: print(loss.item())

    #Initializes weights, trains autoencoder with MSE loss, updates weights, and prints loss periodically.

image_batch,y = get_batch('test')

#Fetches a batch of images and their labels from the test dataset.

image_batch_recon = Autoencoder(image_batch,w)

#Reconstructs the test image batch using the trained autoencoder and weights `w`.

torch.mean((image_batch_recon - image_batch)**2)

#Calculates the mean squared error (MSE) between the reconstructed and original test images.

montage_plot(image_batch[0:25,0,:,:].cpu().detach().numpy())

#Creates and plots a montage of the first 25 images from the test batch, converted back to numpy format.

montage_plot(image_batch_recon[0:25,0,:,:].cpu().detach().numpy())

#Creates and plots a montage of the first 25 reconstructed images from the test batch, converted back to numpy format.

"""
### Generator

"""



latent_size = 64
hidden_size = 256
image_size = 784
b = 1024

#MNIST
# train_set = MNIST('./data', train=True, download=True)
# test_set = MNIST('./data', train=False, download=True)

#KMNIST
#train_set = KMNIST('./data', train=True, download=True)
#test_set = KMNIST('./data', train=False, download=True)

#Fashion MNIST
train_set = FashionMNIST('./data', train=True, download=True)
test_set = FashionMNIST('./data', train=False, download=True)

X = train_set.data.numpy()
X_test = test_set.data.numpy()
Y = train_set.targets.numpy()
Y_test = test_set.targets.numpy()
X = X[:,None,:,:]/255
X_test = X_test[:,None,:,:]/255
X = (X - 0.5)/0.5
X_test = (X_test - 0.5)/0.5

#Converts training and testing data and labels to numpy arrays, normalizes images by scaling pixel values to [-1, 1].

n = 7

index = np.where(Y == n)
X = X[index]
index = np.where(Y_test == n)
X_test = X_test[index]

#Filters the datasets to only include samples where the label is equal to `n`, isolating data for class `n`.

X.shape,Y.shape,X_test.shape,Y_test.shape

###################################################

X = GPU_data(X)
X_test = GPU_data(X_test)
Y = GPU_data(Y)
Y_test = GPU_data(Y_test)

#Converts filtered datasets `X` and `X_test` to GPU tensors without gradients.

x,y = get_batch('train')

#Retrieves a random batch for training, containing inputs `x` and their corresponding labels `y`.



x.shape

montage_plot(x[0:25,0,:,:].detach().cpu().numpy())

#Creates and plots a montage of the first 25 images from the training batch, converted to numpy format.

#D
w0 = GPU(randn_trunc((64,1,4,4)))
w1 = GPU(randn_trunc((128,64,4,4)))
w2 = GPU(randn_trunc((1,6272)))
#G
w3 = GPU(randn_trunc((6272,64)))
w4 = GPU(randn_trunc((128,64,4,4)))
w5 = GPU(randn_trunc((64,1,4,4)))

w = [w0,w1,w2,w3,w4,w5]

#This block initializes weights for a GAN (Generator and Discriminator) using truncated normal distribution.

def D(x,w):
    x = relu(conv2d(x,w[0], stride=(2, 2), padding=(1, 1)))
    x = relu(conv2d(x,w[1], stride=(2, 2), padding=(1, 1)))
    x = x.view(x.size(0), 6272)
    x = linear(x,w[2])
    x = torch.sigmoid(x)
    return x

    #Defines the discriminator function for a GAN, processing input `x` through convolutional and linear layers with ReLU activation, outputting a probability score through a sigmoid function.

def G(x,w):
    x = linear(x,w[3])
    x = x.view(x.size(0), 128, 7, 7)
    x = relu(conv_transpose2d(x,w[4], stride=(2, 2), padding=(1, 1)))
    x = torch.tanh(conv_transpose2d(x,w[5], stride=(2, 2), padding=(1, 1)))
    return x

    #Defines the generator function for a GAN, processing input `x` through linear and transposed convolutional layers with ReLU and tanh activations.

b = 1024

batch_size = b

batch_size

d_optimizer = torch.optim.Adam(w[0:3], lr=0.0002)
g_optimizer = torch.optim.Adam(w[3:], lr=0.0002)

real_labels = (torch.ones(batch_size, 1).cuda())
fake_labels = (torch.zeros(batch_size, 1).cuda())

#Sets up Adam optimizers for discriminator and generator weights separately, with learning rate 0.0002. Defines real and fake labels for training.

num_epochs = 500
batches = X.shape[0]//batch_size
steps = num_epochs*batches

#Calculates the total number of training steps based on the number of epochs and batches.

z1 = (torch.randn(steps,batch_size,latent_size).cuda())
z2 = (torch.randn(steps,batch_size,latent_size).cuda())

#Generates random noise tensors `z1` and `z2` for input to the generator, with shape `(steps, batch_size, latent_size)`, and moves them to the GPU.

for i in range(steps):

    images,y = get_batch('train')

    d_loss = binary_cross_entropy(D(images,w), real_labels) + binary_cross_entropy(D(G(z1[i],w),w), fake_labels)
    d_optimizer.zero_grad()
    d_loss.backward()
    d_optimizer.step()


    g_loss = binary_cross_entropy(D(G(z2[i],w),w), real_labels)
    g_optimizer.zero_grad()
    g_loss.backward()
    g_optimizer.step()


    if i % 200 == 0:
        out = G(z1[np.random.randint(steps)],w)
        montage_plot(out.view(batch_size,1,28,28).detach().cpu().numpy()[0:25,0,:,:])

        #Trains a GAN in alternating steps, updating discriminator and generator weights, and periodically visualizes generated images.





z1[np.random.randint(steps)].shape

noise = GPU_data(torch.randn(1,64))

output = G(noise,w)

output.shape

plot(output[0,0])



















